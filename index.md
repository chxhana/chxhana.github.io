---
layout: default
title: "Home"
---

# Chahana Dahal

I am a computer science PhD student at the **University of Nevada, Las Vegas** (UNLV) working on large language models, knowledge graphs and AI safety.  My research interests focus on unlearning methods for knowledge graphs and responsible AI.  Prior to starting my doctorate I earned a **B.S. in Computer Science** with a minor in Data Science from Westminster University, graduating with a GPA of 3.97.

## Contact

- **Email:** [dahalchahana1@gmail.com](mailto:dahalchahana1@gmail.com)
- **GitHub:** [chxhana](https://github.com/chxhana)
- **LinkedIn:** [chahanadahal](https://www.linkedin.com/in/chahanadahal)
- **Google Scholar:** [scholar profile](https://scholar.google.com)

## Education

- **Ph.D. in Computer Science**, **University of Nevada, Las Vegas** (Sept 2025 – May 2029)
  - Graduate student researcher designing benchmarks to evaluate LLM unlearning on knowledge‑graph facts.
- **B.S. in Computer Science**, **Westminster University** (2021 – 2024) — Minor in Data Science, GPA 3.97
  - Skills: Python, C/C++, Java, PyTorch, Scikit‑learn, SQL, Matplotlib, NumPy, Pandas, Git.
  - Interests: Large language models, knowledge graphs, AI safety, responsible AI, natural language processing.

## Research Publications

Below are some of my publications and ongoing projects.  Papers marked “under review” are currently under peer review; accepted papers are indicated accordingly.

- **“Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE,”** *ICLR 2026 (under review)*.  We explore the balance between architectural simplicity and performance in knowledge‑graph completion.
- **“Federated Retrieval‑Augmented Generation: A Systematic Mapping Study,”** *EMNLP 2025 Findings (accepted)*.  This work surveys federated approaches to retrieval‑augmented generation and provides guidelines for building efficient systems.
- **“HyFical: Hierarchical Hybrid Federated In‑Context Agent Learning for LLMs,”** *ICLR 2026 (under review)*.  We propose a federated framework for training in‑context agents using hierarchical hybrid learning.
- **“Revolutionizing Education through AI‑Powered Inclusive Learning Systems,”** *AAAI 2024 Undergraduate Consortium (accepted)*.  This paper presents inclusive learning systems powered by AI and won an AAAI Undergraduate Scholar award.
- **“AI Agents for Learning: A Survey with Safety and Privacy,”** *IEEE TAI (under review)*.  We survey AI agents for education, highlighting safety and privacy considerations.

## Work Experience

**Graduate Student Researcher — UNLV (Sept 2025 – Present)**  \
Designing a novel benchmark to evaluate large language model unlearning on knowledge‑graph facts.  Building tools to measure utility retention and fairness after unlearning operations.

**Machine Learning Researcher — CoRAL Lab, Arizona State University (Nov 2024 – Present)**  \
Conducted evaluation and robustness analysis for *RelatE*, demonstrating 24 % faster training, 31 % lower inference latency and up to 61 % reduced performance degradation under perturbations on benchmarks such as YAGO3‑10.  Showed that architectural simplicity paired with advanced training achieves competitive performance that outperformed state‑of‑the‑art methods like RotatE and TransE in accuracy, efficiency and robustness.

**Machine Learning Engineer — Omdena (July 2024 – April 2024)**  \
Fine‑tuned multilingual LLMs (mT5, AraGPT2) for Q&A agents and personalized tutoring systems, applying prompt engineering for content understanding and matching.  Built a feedback‑driven fine‑tuning loop using user interaction data to improve recommendation personalization and response precision.

## Honors & Affiliations

- **AAAI Undergraduate Scholar (2024)** — Selected to the AAAI Undergraduate Consortium for research on inclusive learning systems.
- **Google Computer Science Research Mentorship Program Scholar (2023)** — Participated in Google’s CSRMP, working with mentors on responsible AI and knowledge‑graph unlearning.
- **First Generation (Legacy) Scholar** — Recognized for academic excellence as a first‑generation college student.
- **Dean’s List (2021–2024)** — Consistently honoured for outstanding academic performance.
- **Affiliations:** Rewriting the Code; Women in Machine Learning; AnitaB.org; Last Mile.
- **Certifications:** AI with Python (Udacity); AI Agents (Coursera).

## Research Interests

My research aims to develop techniques for safe and responsible use of large language models.  I am particularly interested in:

1. **Unlearning in Large Language Models and Knowledge Graphs** — designing algorithms to selectively remove learned facts while preserving model utility and fairness.  Recent unlearning methods such as Negative Preference Optimization balance unlearning with utility retention【197992972403285†L58-L72】, and Direct Preference Optimization provides a stable, classification‑based approach to aligning LMs with desired preferences【664003164842625†L71-L81】.
2. **Federated and Privacy‑Preserving Machine Learning** — investigating federated training techniques for retrieval‑augmented generation and in‑context agents.
3. **AI Safety and Responsible AI** — ensuring AI systems behave ethically and avoid encoding harmful biases.

---

*This site is powered by GitHub Pages using the minimal theme.  To build your own academic webpage, feel free to adapt this repository.*